import os
import sys
from pathlib import Path

current_file = Path(__file__).resolve()
project_root = current_file.parent.parent.parent  # knowledge-base -> apps -> openrd-master
sys.path.insert(0, str(project_root))

print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
print(f"ğŸ“ å½“å‰ç›®å½•: {current_file.parent}")

try:
    from config import config
    print(f"âœ… æˆåŠŸå¯¼å…¥é…ç½®æ¨¡å—")
    
    # éªŒè¯é…ç½®
    config.chroma.validate()
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥é…ç½®æ¨¡å—å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•æœ‰ config/__init__.py æ–‡ä»¶")
    print(f"ğŸ’¡ Pythonè·¯å¾„: {sys.path}")
    sys.exit(1)
except ValueError as e:
    print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
    sys.exit(1)

import json
import time
import chromadb
import PyPDF2
from docx import Document
from datetime import datetime
from typing import List, Dict
import re
import hashlib
import langdetect
from langdetect import detect, DetectorFactory

DetectorFactory.seed = 0

class FSHDBatchProcessor:
    """åŸºç¡€çš„äº‘ç«¯FSHDçŸ¥è¯†åº“å¤„ç†å™¨"""
    def __init__(self, cloud_api_key: str, tenant_id: str, database_name: str = "FSHD"):
        """åˆå§‹åŒ–äº‘ç«¯FSHDçŸ¥è¯†åº“å¤„ç†å™¨"""
        self.client = chromadb.CloudClient(
            api_key=cloud_api_key,
            tenant=tenant_id,
            database=database_name
        )
        self.collection = self.client.get_or_create_collection("fshd_knowledge_base")
        print("ğŸš€ äº‘ç«¯FSHDçŸ¥è¯†åº“å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ“ è¿æ¥è‡³: {database_name} | ç§Ÿæˆ·: {tenant_id}")
    
    def generate_short_id(self, category: str, filename: str, language: str, chunk_index: int) -> str:
        """ç”ŸæˆçŸ­IDä»¥é¿å…é…é¢é™åˆ¶"""
        base_string = f"{category}_{filename}_{language}_{chunk_index}"
        return hashlib.md5(base_string.encode()).hexdigest()[:16]
    
    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            sample_text = text[:1000] if len(text) > 1000 else text
            if len(sample_text.strip()) < 10:  
                return "unknown"
            return detect(sample_text)
        except:
            return "unknown"
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict:
        """ä»PDFæå–æ–‡æœ¬"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    text += page_text + "\n"
            return {
                "text": text,
                "success": True,
                "pages": total_pages,
                "file_type": "pdf"
            }
        except Exception as e:
            print(f"âŒ è¯»å–PDFå¤±è´¥ {pdf_path}: {e}")
            return {"success": False, "error": str(e)}
    
    def extract_text_from_docx(self, docx_path: str) -> Dict:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        text = ""
        try:
            doc = Document(docx_path)
            total_paragraphs = len(doc.paragraphs)
            for para in doc.paragraphs:
                text += para.text + "\n"
            return {
                "text": text,
                "success": True,
                "paragraphs": total_paragraphs,
                "file_type": "docx"
            }
        except Exception as e:
            print(f"âŒ è¯»å–Wordæ–‡æ¡£å¤±è´¥ {docx_path}: {e}")
            return {"success": False, "error": str(e)}
    
    def extract_text(self, file_path: str) -> Dict:
        """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
        if file_path.lower().endswith('.pdf'):
            return self.extract_text_from_pdf(file_path)
        elif file_path.lower().endswith(('.docx', '.doc')):
            return self.extract_text_from_docx(file_path)
        else:
            return {"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}
    
    def smart_chunking(self, text: str, language: str, chunk_size: int = 500) -> List[str]:
        """æ ¹æ®è¯­è¨€æ™ºèƒ½åˆ†å—"""
        if language == 'en':
            # è‹±æ–‡åˆ†å—ç­–ç•¥
            sentences = re.split(r'[.!?]+', text)
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                clean_sentence = sentence.strip()
                if len(clean_sentence) == 0:
                    continue
                    
                if len(current_chunk) + len(clean_sentence) <= chunk_size:
                    current_chunk += clean_sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = clean_sentence + ". "
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            # ä¸­æ–‡åˆ†å—ç­–ç•¥
            paragraphs = re.split(r'\n\s*\n', text)
            chunks = []
            current_chunk = ""
            
            for paragraph in paragraphs:
                clean_para = re.sub(r'\s+', ' ', paragraph.strip())
                if len(clean_para) == 0:
                    continue
                    
                if len(current_chunk) + len(clean_para) <= chunk_size:
                    current_chunk += clean_para + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = clean_para + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        
        return chunks
    
    def find_all_documents(self, root_path: str) -> List[Dict]:
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰PDFå’ŒWordæ–‡æ¡£"""
        documents = []
        
        for root, dirs, files in os.walk(root_path):
            # è·³è¿‡ç³»ç»Ÿæ–‡ä»¶å¤¹å’Œä¸´æ—¶æ–‡ä»¶å¤¹
            dirs[:] = [d for d in dirs if not d.startswith('.') and not d.startswith('_')]
            
            for file in files:
                if file.lower().endswith(('.pdf', '.docx', '.doc')):
                    full_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ç”¨äºåˆ†ç±»
                    relative_path = os.path.relpath(root, root_path)
                    category = relative_path if relative_path != '.' else os.path.basename(root_path)
                    
                    documents.append({
                        'path': full_path,
                        'filename': file,
                        'category': category,
                        'folder_structure': relative_path
                    })
        
        return documents
    
    def process_document(self, doc_info: Dict) -> int:
        """å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶ä¸Šä¼ åˆ°äº‘ç«¯"""
        file_path = doc_info['path']
        category = doc_info['category']
        filename = doc_info['filename']
        
        print(f"\nğŸ”„ å¤„ç†: {filename}")
        print(f"    ğŸ“ åˆ†ç±»: {category}")
        
        # æå–æ–‡æœ¬
        result = self.extract_text(file_path)
        if not result["success"]:
            print(f"   âŒ æ–‡æœ¬æå–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return 0
        
        text = result["text"]
        file_type = result["file_type"]
        
        if not text or len(text.strip()) < 50:
            print("   âš ï¸  æ–‡æ¡£å†…å®¹è¿‡å°‘ï¼Œè·³è¿‡å¤„ç†")
            return 0
        
        # æ£€æµ‹è¯­è¨€
        language = self.detect_language(text)
        
        # æ ¹æ®è¯­è¨€åˆ†å—
        chunks = self.smart_chunking(text, language)
        
        print(f"   ğŸ“ ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å— | è¯­è¨€: {language}")
        
        # å‡†å¤‡æ•°æ® - åˆ†æ‰¹ä¸Šä¼ ä»¥é¿å…é…é¢é™åˆ¶
        batch_size = 30  # æ¯æ‰¹ä¸Šä¼ 30ä¸ªæ–‡æ¡£å—
        total_uploaded = 0
        
        for batch_start in range(0, len(chunks), batch_size):
            batch_end = min(batch_start + batch_size, len(chunks))
            batch_chunks = chunks[batch_start:batch_end]
            
            documents = []
            metadatas = []
            ids = []
            
            for j, chunk in enumerate(batch_chunks):
                if len(chunk.strip()) > 30:
                    documents.append(chunk)
                    metadatas.append({
                        "category": category,
                        "doc_type": "åŒ»å­¦æ–‡æ¡£",
                        "source_file": filename,
                        "file_type": file_type,
                        "language": language,
                        "chunk_index": batch_start + j,
                        "folder_path": doc_info['folder_structure'],
                        "full_path": file_path
                    })
                    # ä½¿ç”¨çŸ­IDé¿å…é…é¢é™åˆ¶
                    short_id = self.generate_short_id(category, filename, language, batch_start + j)
                    ids.append(short_id)
            
            if documents:
                try:
                    # å…³é”®ä¿®æ”¹ï¼šåˆ†æ‰¹ä¸Šä¼ åˆ°äº‘ç«¯
                    self.collection.add(
                        documents=documents,
                        metadatas=metadatas,
                        ids=ids
                    )
                    total_uploaded += len(documents)
                    print(f"   âœ… æ‰¹æ¬¡ {batch_start//batch_size + 1}: æˆåŠŸä¸Šä¼  {len(documents)} ä¸ªæ–‡æœ¬å—")
                    
                    # æ·»åŠ å°å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶
                    time.sleep(0.2)
                    
                except Exception as e:
                    print(f"   âŒ æ‰¹æ¬¡ä¸Šä¼ å¤±è´¥: {e}")
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        
        if total_uploaded > 0:
            print(f"   ğŸ‰ æ€»è®¡ä¸Šä¼ : {total_uploaded} ä¸ªæ–‡æœ¬å—åˆ°äº‘ç«¯ï¼")
            return total_uploaded
        else:
            print("   âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—å¯æ·»åŠ ")
            return 0
    
    def get_collection_stats(self):
        """è·å–äº‘ç«¯çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼Œé¿å…é…é¢é™åˆ¶ï¼‰"""
        try:
            count = self.collection.count()
            print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡: æ€»æ–‡æ¡£æ•° = {count}")
            
            # ä½¿ç”¨æ›´å°çš„limité¿å…é…é¢é—®é¢˜
            sample_limit = min(50, count)  # æœ€å¤šåªå–50æ¡æ ·æœ¬ï¼Œé¿å…è¶…è¿‡300æ¡é™åˆ¶
            language_dist = {}
            category_dist = {}
            
            if sample_limit > 0:
                try:
                    # åˆ†æ‰¹è·å–æ•°æ®
                    sample_data = self.collection.peek(limit=sample_limit)
                    
                    for meta in sample_data.get("metadatas", []):
                        lang = meta.get("language", "unknown")
                        category = meta.get("category", "unknown")
                        
                        # ç®€åŒ–åˆ†ç±»åç§°ï¼ˆå»æ‰è·¯å¾„ï¼‰
                        if isinstance(category, str) and '\\' in category:
                            # åªå–æœ€åä¸€éƒ¨åˆ†
                            category_parts = category.split('\\')
                            category = category_parts[-1]
                        
                        language_dist[lang] = language_dist.get(lang, 0) + 1
                        category_dist[category] = category_dist.get(category, 0) + 1
                        
                except Exception as e:
                    print(f"âš ï¸ é‡‡æ ·ç»Ÿè®¡å¤±è´¥: {str(e)[:80]}")
                    # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ç»Ÿè®¡
                    return {
                        "total_chunks": count,
                        "language_distribution": {"unknown": count},
                        "category_distribution": {"unknown": count}
                    }
            else:
                # ç©ºæ•°æ®åº“
                language_dist = {}
                category_dist = {}
            
            return {
                "total_chunks": count,
                "language_distribution": language_dist,
                "category_distribution": category_dist
            }
            
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: {str(e)[:100]}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return {
                "total_chunks": 0,
                "language_distribution": {},
                "category_distribution": {}
            }

class OptimizedFSHDUploader(FSHDBatchProcessor):
    """ä¼˜åŒ–ç‰ˆä¸Šä¼ å™¨ï¼Œæ·»åŠ è¿›åº¦ä¿å­˜å’Œé”™è¯¯æ¢å¤"""
    
    def __init__(self, cloud_api_key: str, tenant_id: str, database_name: str = "FSHD"):
        super().__init__(cloud_api_key, tenant_id, database_name)
        self.progress_file = "upload_progress.json"
        self.load_progress()
    
    def load_progress(self):
        """åŠ è½½ä¸Šä¼ è¿›åº¦"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    self.progress = json.load(f)
                print(f"ğŸ“š åŠ è½½è¿›åº¦: å·²å¤„ç† {len(self.progress.get('processed_files', []))} ä¸ªæ–‡ä»¶")
            else:
                self.progress = {
                    "start_time": datetime.now().isoformat(),
                    "processed_files": [],
                    "failed_files": [],
                    "total_chunks": 0,
                    "last_checkpoint": None
                }
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿›åº¦å¤±è´¥: {e}")
            self.progress = {"processed_files": [], "failed_files": []}
    
    def save_progress(self):
        """ä¿å­˜ä¸Šä¼ è¿›åº¦"""
        try:
            self.progress["last_checkpoint"] = datetime.now().isoformat()
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def is_file_processed(self, file_path: str):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        return file_path in self.progress.get("processed_files", [])
    
    def process_entire_knowledge_base_safe(self, root_path: str):
        """å®‰å…¨å¤„ç†æ•´ä¸ªçŸ¥è¯†åº“ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
        if not os.path.exists(root_path):
            print(f"âŒ çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {root_path}")
            return
        
        print("ğŸ” æ‰«æçŸ¥è¯†åº“æ–‡ä»¶å¤¹ç»“æ„...")
        all_documents = self.find_all_documents(root_path)
        
        if not all_documents:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•PDFæˆ–Wordæ–‡æ¡£")
            return
        
        print(f"\nğŸ“Š æ‰«æå®Œæˆï¼æ‰¾åˆ° {len(all_documents)} ä¸ªæ–‡æ¡£")
        
        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        pending_documents = []
        for doc in all_documents:
            if not self.is_file_processed(doc['path']):
                pending_documents.append(doc)
        
        print(f"ğŸ“‹ å¾…å¤„ç†æ–‡ä»¶: {len(pending_documents)} ä¸ª (è·³è¿‡ {len(all_documents)-len(pending_documents)} ä¸ªå·²å¤„ç†)")
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for doc in pending_documents:
            category = doc['category']
            category_stats[category] = category_stats.get(category, 0) + 1
        
        if category_stats:
            print("\nğŸ“‚ å¾…å¤„ç†æ–‡æ¡£åˆ†ç±»åˆ†å¸ƒ:")
            for category, count in sorted(category_stats.items()):
                print(f"   {category}: {count} ä¸ªæ–‡æ¡£")
        
        total_chunks = self.progress.get("total_chunks", 0)
        processed_files = len(self.progress.get("processed_files", []))
        
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹æ‰¹é‡ä¸Šä¼ åˆ°äº‘ç«¯çŸ¥è¯†åº“ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰")
        print(f"ğŸ“ˆ è¿›åº¦: {processed_files}/{len(all_documents)} æ–‡ä»¶")
        print(f"{'='*60}")
        
        batch_counter = 0
        for i, doc_info in enumerate(pending_documents, 1):
            file_path = doc_info['path']
            
            print(f"\n[{processed_files + i}/{len(all_documents)}] ", end="")
            
            try:
                chunks_added = self.process_document(doc_info)
                
                if chunks_added > 0:
                    total_chunks += chunks_added
                    processed_files += 1
                    self.progress["processed_files"].append(file_path)
                    self.progress["total_chunks"] = total_chunks
                    
                    # æ¯å¤„ç†5ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
                    if i % 5 == 0:
                        self.save_progress()
                        print(f"   ğŸ’¾ è¿›åº¦å·²ä¿å­˜ ({processed_files}/{len(all_documents)})")
                else:
                    self.progress.setdefault("failed_files", []).append({
                        "path": file_path,
                        "reason": "æ— æœ‰æ•ˆæ–‡æœ¬å—",
                        "time": datetime.now().isoformat()
                    })
                    
            except Exception as e:
                error_msg = f"å¤„ç†å¤±è´¥: {str(e)[:100]}"
                print(f"   âŒ {error_msg}")
                self.progress.setdefault("failed_files", []).append({
                    "path": file_path,
                    "reason": error_msg,
                    "time": datetime.now().isoformat()
                })
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼Œä¸ä¸­æ–­
                continue
            
            # æ¯å¤„ç†10ä¸ªæ–‡ä»¶ç¨ä½œä¼‘æ¯ï¼Œé¿å…é€Ÿç‡é™åˆ¶
            batch_counter += 1
            if batch_counter >= 10:
                time.sleep(3)
                batch_counter = 0
        
        # æœ€ç»ˆä¿å­˜è¿›åº¦
        self.save_progress()
        
        print(f"\n{'ğŸ‰' * 30}")
        print("äº‘ç«¯çŸ¥è¯†åº“æ‰¹é‡ä¸Šä¼ å®Œæˆï¼")
        print(f"{'ğŸ‰' * 30}")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   ğŸ“ æ‰«ææ–‡æ¡£æ€»æ•°: {len(all_documents)} ä¸ª")
        print(f"   âœ… æˆåŠŸå¤„ç†: {processed_files} ä¸ªæ–‡æ¡£")
        print(f"   âŒ å¤±è´¥: {len(self.progress.get('failed_files', []))} ä¸ªæ–‡æ¡£")
        print(f"   ğŸ§© ç”Ÿæˆæ–‡æœ¬å—: {total_chunks} ä¸ª")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨ä¿®å¤åçš„æ–¹æ³•ï¼‰
        try:
            stats = self.get_collection_stats()
            print(f"\nğŸ“ˆ äº‘ç«¯çŸ¥è¯†åº“æ€»ä½“ç»Ÿè®¡:")
            print(f"   ğŸ§© æ€»æ–‡æœ¬å—æ•°: {stats['total_chunks']}")
            if stats.get('language_distribution'):
                print(f"   ğŸŒ è¯­è¨€åˆ†å¸ƒ: {stats['language_distribution']}")
            if stats.get('category_distribution'):
                print(f"   ğŸ“‚ åˆ†ç±»æ•°é‡: {len(stats['category_distribution'])} ä¸ª")
        except Exception as e:
            print(f"\nâš ï¸  è·å–æœ€ç»ˆç»Ÿè®¡å¤±è´¥ï¼ˆä¸å½±å“æ•°æ®ä¸Šä¼ ï¼‰: {str(e)[:80]}")
            print(f"   æ‚¨å¯ä»¥é€šè¿‡æŸ¥è¯¢APIéªŒè¯æ•°æ®: curl http://localhost:5000/api/stats")
        
        # å¦‚æœæœ‰å¤±è´¥çš„æ–‡ä»¶
        failed_files = self.progress.get("failed_files", [])
        if failed_files:
            print(f"\nâš ï¸  å¤±è´¥æ–‡ä»¶åˆ—è¡¨ ({len(failed_files)} ä¸ª):")
            for fail in failed_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                filename = os.path.basename(fail['path']) if 'path' in fail else 'æœªçŸ¥æ–‡ä»¶'
                print(f"   - {filename}: {fail.get('reason', 'æœªçŸ¥åŸå› ')}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"upload_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            # å°è¯•è·å–æœ€ç»ˆç»Ÿè®¡ç”¨äºæŠ¥å‘Š
            try:
                final_stats = self.get_collection_stats()
            except:
                final_stats = {"total_chunks": "è·å–å¤±è´¥", "error": "é…é¢é™åˆ¶"}
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "summary": {
                        "total_documents": len(all_documents),
                        "processed_success": processed_files,
                        "failed": len(failed_files),
                        "total_chunks": total_chunks,
                        "database_count": final_stats.get('total_chunks', 'æœªçŸ¥')
                    },
                    "progress": self.progress,
                    "stats": final_stats
                }, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print(f"\nâš ï¸  ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

def main_optimized():
    """ä¼˜åŒ–ç‰ˆä¸»å‡½æ•°"""
    # ğŸ¯ ä½¿ç”¨æ‚¨çš„äº‘ç«¯å‡­æ®
    CLOUD_API_KEY = "ck-G1qMBnQAHG1B1xZN8b1fHzcjbq1TxdbSFsNofzGaZT5c"
    TENANT_ID = "bf4422ea-4e6b-4f9b-8682-bc9f92d22f04"
    
    # åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆä¸Šä¼ å™¨
    uploader = OptimizedFSHDUploader(
        cloud_api_key=CLOUD_API_KEY,
        tenant_id=TENANT_ID,
        database_name="FSHD"
    )
    
    # çŸ¥è¯†åº“æ ¹è·¯å¾„
    knowledge_base_path = r"C:\yoyo\openrd-master\FSHD_çŸ¥è¯†åº“"
    
    print("ğŸ¯ å¼€å§‹å®‰å…¨æ‰¹é‡ä¸Šä¼ FSHDçŸ¥è¯†åº“åˆ°äº‘ç«¯")
    print(f"ğŸ“ æœ¬åœ°çŸ¥è¯†åº“ä½ç½®: {knowledge_base_path}")
    print("ğŸ›¡ï¸  æ¨¡å¼: æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€é”™è¯¯æ¢å¤ã€è¿›åº¦ä¿å­˜")
    print("â³ ä¸Šä¼ å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")
    
    # å®‰å…¨å¤„ç†æ•´ä¸ªçŸ¥è¯†åº“
    uploader.process_entire_knowledge_base_safe(knowledge_base_path)

if __name__ == "__main__":
    main_optimized()