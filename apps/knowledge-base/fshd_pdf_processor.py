import chromadb
import os
import PyPDF2
from typing import List, Dict
import re
import langdetect
from langdetect import detect, DetectorFactory

DetectorFactory.seed = 0

class FSHDPDFProcessor:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection("fshd_knowledge_base")
        print("ğŸš€ FSHDå¤šè¯­è¨€çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼")
    
    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            sample_text = text[:1000] if len(text) > 1000 else text
            if len(sample_text.strip()) < 10:  
                return "unknown"
            return detect(sample_text)
        except:
            return "unknown"
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict:
        """ä»PDFæå–æ–‡æœ¬å¹¶æ£€æµ‹è¯­è¨€"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    text += page_text + "\n"
                    print(f"    ğŸ“„ é¡µé¢ {page_num+1}/{total_pages} æå–å®Œæˆ")
            
            language = self.detect_language(text)
            return {
                "text": text,
                "language": language,
                "success": True,
                "pages": total_pages
            }
        except Exception as e:
            print(f"âŒ è¯»å–PDFå¤±è´¥ {pdf_path}: {e}")
            return {"success": False, "error": str(e)}
    
    def smart_chunking(self, text: str, language: str, chunk_size: int = 500) -> List[str]:
        """æ ¹æ®è¯­è¨€æ™ºèƒ½åˆ†å—"""
        if language == 'en':
            # è‹±æ–‡åˆ†å—ç­–ç•¥
            sentences = re.split(r'[.!?]+', text)
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                clean_sentence = sentence.strip()
                if len(clean_sentence) == 0:
                    continue
                    
                if len(current_chunk) + len(clean_sentence) <= chunk_size:
                    current_chunk += clean_sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = clean_sentence + ". "
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            # ä¸­æ–‡åˆ†å—ç­–ç•¥
            paragraphs = re.split(r'\n\s*\n', text)
            chunks = []
            current_chunk = ""
            
            for paragraph in paragraphs:
                clean_para = re.sub(r'\s+', ' ', paragraph.strip())
                if len(clean_para) == 0:
                    continue
                    
                if len(current_chunk) + len(clean_para) <= chunk_size:
                    current_chunk += clean_para + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = clean_para + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        
        return chunks
    
    def process_single_category(self, folder_path: str, category: str):
        """å¤„ç†å•ä¸ªåˆ†ç±»çš„æ‰€æœ‰PDF"""
        if not os.path.exists(folder_path):
            print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return 0
        
        pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]
        
        if not pdf_files:
            print(f"ğŸ“ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰PDFæ–‡ä»¶: {folder_path}")
            return 0
        
        print(f"\n{'='*50}")
        print(f"ğŸ“‚ å¼€å§‹å¤„ç†åˆ†ç±»: {category}")
        print(f"ğŸ“„ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        print(f"ğŸ“ æ–‡ä»¶å¤¹è·¯å¾„: {folder_path}")
        print(f"{'='*50}")
        
        total_chunks = 0
        
        for i, pdf_file in enumerate(pdf_files, 1):
            pdf_path = os.path.join(folder_path, pdf_file)
            print(f"\n[{i}/{len(pdf_files)}] ğŸ”„ å¤„ç†: {pdf_file}")
            
            # æå–æ–‡æœ¬å’Œè¯­è¨€ä¿¡æ¯
            result = self.extract_text_from_pdf(pdf_path)
            if not result["success"]:
                continue
            
            text = result["text"]
            language = result["language"]
            pages = result["pages"]
            
            if not text or len(text.strip()) < 50:
                print("   âš ï¸  æ–‡æ¡£å†…å®¹è¿‡å°‘ï¼Œè·³è¿‡å¤„ç†")
                continue
            
            # æ ¹æ®è¯­è¨€åˆ†å—
            chunks = self.smart_chunking(text, language)
            print(f"   ğŸ“ ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å— | è¯­è¨€: {language} | é¡µæ•°: {pages}")
            
            # å‡†å¤‡æ•°æ®
            documents = []
            metadatas = []
            ids = []
            
            for j, chunk in enumerate(chunks):
                if len(chunk.strip()) > 30:
                    documents.append(chunk)
                    metadatas.append({
                        "category": category,
                        "doc_type": "åŒ»å­¦æ–‡æ¡£",
                        "source_file": pdf_file,
                        "language": language,
                        "chunk_index": j,
                        "total_pages": pages
                    })
                    ids.append(f"{category}_{pdf_file}_{language}_{j}")
            
            if documents:
                self.collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                total_chunks += len(documents)
                print(f"   âœ… æˆåŠŸæ·»åŠ : {len(documents)} ä¸ªæ–‡æœ¬å—")
            else:
                print("   âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—å¯æ·»åŠ ")
        
        return total_chunks
    
    def search_fshd_knowledge(self, question: str, n_results: int = 3, language_filter: str = None):
        """æœç´¢FSHDçŸ¥è¯†åº“"""
        # å¯é€‰çš„è¯­è¨€è¿‡æ»¤
        where_filter = None
        if language_filter:
            where_filter = {"language": language_filter}
        
        # æ‰§è¡Œå‘é‡æœç´¢
        results = self.collection.query(
            query_texts=[question],
            n_results=n_results,
            where=where_filter
        )
        return results
    
    def search_knowledge(self, question: str, n_results: int = 3, language_filter: str = None):
        """æœç´¢FSHDçŸ¥è¯†åº“ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.search_fshd_knowledge(question, n_results, language_filter)
    
    def get_collection_stats(self):
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        count = self.collection.count()
        all_metadatas = self.collection.get()["metadatas"]
        
        language_dist = {}
        category_dist = {}
        
        for meta in all_metadatas:
            lang = meta.get("language", "unknown")
            category = meta.get("category", "unknown")
            
            language_dist[lang] = language_dist.get(lang, 0) + 1
            category_dist[category] = category_dist.get(category, 0) + 1
        
        return {
            "total_chunks": count,
            "language_distribution": language_dist,
            "category_distribution": category_dist
        }

def main():
    """ä¸»å‡½æ•° - ä¸“é—¨å¤„ç†ç–¾ç—…å®šä¹‰å’Œç§‘æ™®åˆ†ç±»"""
    processor = FSHDPDFProcessor()
    
    # ä¸“é—¨å¤„ç†ç–¾ç—…å®šä¹‰å’Œç§‘æ™®åˆ†ç±» - ä½¿ç”¨ç»å¯¹è·¯å¾„
    folder_path = r"C:\yoyo\openrd-master\FSHD_çŸ¥è¯†åº“\01.ç–¾ç—…å®šä¹‰å’Œç§‘æ™®\ç¬¬ä¸€æ‰¹ï¼š2025å¹´3æœˆ31æ—¥"
    category = "ç–¾ç—…å®šä¹‰å’Œç§‘æ™®"
    
    print("ğŸ¯ å¼€å§‹å¤„ç†: ç–¾ç—…å®šä¹‰å’Œç§‘æ™®åˆ†ç±»")
    print(f"ğŸ“ æ–‡æ¡£ä½ç½®: {folder_path}")
    print("â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")
    
    # å¤„ç†è¯¥åˆ†ç±»
    total_chunks = processor.process_single_category(folder_path, category)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_collection_stats()
    
    print(f"\n{'ğŸ‰' * 20}")
    print("çŸ¥è¯†åº“å¤„ç†å®Œæˆï¼")
    print(f"{'ğŸ‰' * 20}")
    print(f"ğŸ“Š æœ¬æ¬¡å¤„ç†ç»Ÿè®¡:")
    print(f"   ğŸ“ åˆ†ç±»: {category}")
    print(f"   ğŸ“„ å¤„ç†çš„PDFæ•°é‡: 9ä¸ª (8è‹±æ–‡ + 1ä¸­æ–‡)")
    print(f"   ğŸ§© ç”Ÿæˆçš„æ–‡æœ¬å—: {total_chunks} ä¸ª")
    print(f"\nğŸ“ˆ çŸ¥è¯†åº“æ€»ä½“ç»Ÿè®¡:")
    print(f"   ğŸ§© æ€»æ–‡æœ¬å—æ•°: {stats['total_chunks']}")
    print(f"   ğŸŒ è¯­è¨€åˆ†å¸ƒ: {stats['language_distribution']}")
    print(f"   ğŸ“‚ åˆ†ç±»åˆ†å¸ƒ: {stats['category_distribution']}")
    
    # æµ‹è¯•æœç´¢
    print(f"\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
    test_questions = [
        "What is Facioscapulohumeral Muscular Dystrophy?",
        "FSHDçš„ä¸»è¦ç—‡çŠ¶æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for question in test_questions:
        results = processor.search_knowledge(question, n_results=2)
        print(f"\nâ“ é—®é¢˜: {question}")
        print(f"ğŸ“‹ æ‰¾åˆ° {len(results['documents'][0])} ä¸ªç›¸å…³ç»“æœ")
        for j, doc in enumerate(results['documents'][0]):
            print(f"   {j+1}. {doc[:100]}...")
            print(f"      è¯­è¨€: {results['metadatas'][0][j]['language']}")
            print(f"      æ¥æº: {results['metadatas'][0][j]['source_file']}")

if __name__ == "__main__":
    main()