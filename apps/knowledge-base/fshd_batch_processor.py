import chromadb
import os
import PyPDF2
from docx import Document
from typing import List, Dict
import re
import langdetect
from langdetect import detect, DetectorFactory

DetectorFactory.seed = 0

class FSHDBatchProcessor:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection("fshd_knowledge_base")
        print("ğŸš€ FSHDæ‰¹é‡çŸ¥è¯†åº“å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼")
    
    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            sample_text = text[:1000] if len(text) > 1000 else text
            if len(sample_text.strip()) < 10:  
                return "unknown"
            return detect(sample_text)
        except:
            return "unknown"
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict:
        """ä»PDFæå–æ–‡æœ¬"""
        text = ""
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                total_pages = len(reader.pages)
                for page_num, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    text += page_text + "\n"
            return {
                "text": text,
                "success": True,
                "pages": total_pages,
                "file_type": "pdf"
            }
        except Exception as e:
            print(f"âŒ è¯»å–PDFå¤±è´¥ {pdf_path}: {e}")
            return {"success": False, "error": str(e)}
    
    def extract_text_from_docx(self, docx_path: str) -> Dict:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        text = ""
        try:
            doc = Document(docx_path)
            total_paragraphs = len(doc.paragraphs)
            for para in doc.paragraphs:
                text += para.text + "\n"
            return {
                "text": text,
                "success": True,
                "paragraphs": total_paragraphs,
                "file_type": "docx"
            }
        except Exception as e:
            print(f"âŒ è¯»å–Wordæ–‡æ¡£å¤±è´¥ {docx_path}: {e}")
            return {"success": False, "error": str(e)}
    
    def extract_text(self, file_path: str) -> Dict:
        """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
        if file_path.lower().endswith('.pdf'):
            return self.extract_text_from_pdf(file_path)
        elif file_path.lower().endswith(('.docx', '.doc')):
            return self.extract_text_from_docx(file_path)
        else:
            return {"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}
    
    def smart_chunking(self, text: str, language: str, chunk_size: int = 500) -> List[str]:
        """æ ¹æ®è¯­è¨€æ™ºèƒ½åˆ†å—"""
        if language == 'en':
            # è‹±æ–‡åˆ†å—ç­–ç•¥
            sentences = re.split(r'[.!?]+', text)
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                clean_sentence = sentence.strip()
                if len(clean_sentence) == 0:
                    continue
                    
                if len(current_chunk) + len(clean_sentence) <= chunk_size:
                    current_chunk += clean_sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = clean_sentence + ". "
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            # ä¸­æ–‡åˆ†å—ç­–ç•¥
            paragraphs = re.split(r'\n\s*\n', text)
            chunks = []
            current_chunk = ""
            
            for paragraph in paragraphs:
                clean_para = re.sub(r'\s+', ' ', paragraph.strip())
                if len(clean_para) == 0:
                    continue
                    
                if len(current_chunk) + len(clean_para) <= chunk_size:
                    current_chunk += clean_para + "\n\n"
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = clean_para + "\n\n"
            
            if current_chunk:
                chunks.append(current_chunk.strip())
        
        return chunks
    
    def find_all_documents(self, root_path: str) -> List[Dict]:
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰PDFå’ŒWordæ–‡æ¡£"""
        documents = []
        
        for root, dirs, files in os.walk(root_path):
            # è·³è¿‡ç³»ç»Ÿæ–‡ä»¶å¤¹å’Œä¸´æ—¶æ–‡ä»¶å¤¹
            dirs[:] = [d for d in dirs if not d.startswith('.') and not d.startswith('_')]
            
            for file in files:
                if file.lower().endswith(('.pdf', '.docx', '.doc')):
                    full_path = os.path.join(root, file)
                    # è®¡ç®—ç›¸å¯¹è·¯å¾„ç”¨äºåˆ†ç±»
                    relative_path = os.path.relpath(root, root_path)
                    category = relative_path if relative_path != '.' else os.path.basename(root_path)
                    
                    documents.append({
                        'path': full_path,
                        'filename': file,
                        'category': category,
                        'folder_structure': relative_path
                    })
        
        return documents
    
    def process_document(self, doc_info: Dict) -> int:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        file_path = doc_info['path']
        category = doc_info['category']
        filename = doc_info['filename']
        
        print(f"\nğŸ”„ å¤„ç†: {filename}")
        print(f"    ğŸ“ åˆ†ç±»: {category}")
        print(f"    ğŸ“‚ è·¯å¾„: {file_path}")
        
        # æå–æ–‡æœ¬
        result = self.extract_text(file_path)
        if not result["success"]:
            print(f"   âŒ æ–‡æœ¬æå–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return 0
        
        text = result["text"]
        file_type = result["file_type"]
        
        if not text or len(text.strip()) < 50:
            print("   âš ï¸  æ–‡æ¡£å†…å®¹è¿‡å°‘ï¼Œè·³è¿‡å¤„ç†")
            return 0
        
        # æ£€æµ‹è¯­è¨€
        language = self.detect_language(text)
        
        # æ ¹æ®è¯­è¨€åˆ†å—
        chunks = self.smart_chunking(text, language)
        
        # æ·»åŠ æ–‡ä»¶ç‰¹å®šä¿¡æ¯
        file_info = ""
        if file_type == "pdf":
            file_info = f"é¡µæ•°: {result['pages']}"
        elif file_type == "docx":
            file_info = f"æ®µè½: {result['paragraphs']}"
        
        print(f"   ğŸ“ ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å— | è¯­è¨€: {language} | {file_info}")
        
        # å‡†å¤‡æ•°æ®
        documents = []
        metadatas = []
        ids = []
        
        for j, chunk in enumerate(chunks):
            if len(chunk.strip()) > 30:
                documents.append(chunk)
                metadatas.append({
                    "category": category,
                    "doc_type": "åŒ»å­¦æ–‡æ¡£",
                    "source_file": filename,
                    "file_type": file_type,
                    "language": language,
                    "chunk_index": j,
                    "folder_path": doc_info['folder_structure'],
                    "full_path": file_path
                })
                ids.append(f"{category}_{filename}_{language}_{j}")
        
        if documents:
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            print(f"   âœ… æˆåŠŸæ·»åŠ : {len(documents)} ä¸ªæ–‡æœ¬å—")
            return len(documents)
        else:
            print("   âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—å¯æ·»åŠ ")
            return 0
    
    def process_entire_knowledge_base(self, root_path: str):
        """å¤„ç†æ•´ä¸ªçŸ¥è¯†åº“æ–‡ä»¶å¤¹"""
        if not os.path.exists(root_path):
            print(f"âŒ çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨: {root_path}")
            return
        
        print("ğŸ” æ‰«æçŸ¥è¯†åº“æ–‡ä»¶å¤¹ç»“æ„...")
        all_documents = self.find_all_documents(root_path)
        
        if not all_documents:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•PDFæˆ–Wordæ–‡æ¡£")
            return
        
        print(f"\nğŸ“Š æ‰«æå®Œæˆï¼æ‰¾åˆ° {len(all_documents)} ä¸ªæ–‡æ¡£")
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for doc in all_documents:
            category = doc['category']
            category_stats[category] = category_stats.get(category, 0) + 1
        
        print("\nğŸ“‚ æ–‡æ¡£åˆ†ç±»åˆ†å¸ƒ:")
        for category, count in category_stats.items():
            print(f"   {category}: {count} ä¸ªæ–‡æ¡£")
        
        total_chunks = 0
        processed_files = 0
        
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æ¡£...")
        print(f"{'='*60}")
        
        for i, doc_info in enumerate(all_documents, 1):
            print(f"\n[{i}/{len(all_documents)}] ", end="")
            chunks_added = self.process_document(doc_info)
            total_chunks += chunks_added
            if chunks_added > 0:
                processed_files += 1
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = self.get_collection_stats()
        
        print(f"\n{'ğŸ‰' * 30}")
        print("æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"{'ğŸ‰' * 30}")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   ğŸ“ æ‰«ææ–‡æ¡£æ€»æ•°: {len(all_documents)} ä¸ª")
        print(f"   âœ… æˆåŠŸå¤„ç†: {processed_files} ä¸ªæ–‡æ¡£")
        print(f"   ğŸ§© ç”Ÿæˆæ–‡æœ¬å—: {total_chunks} ä¸ª")
        print(f"\nğŸ“ˆ çŸ¥è¯†åº“æ€»ä½“ç»Ÿè®¡:")
        print(f"   ğŸ§© æ€»æ–‡æœ¬å—æ•°: {stats['total_chunks']}")
        print(f"   ğŸŒ è¯­è¨€åˆ†å¸ƒ: {stats['language_distribution']}")
        print(f"   ğŸ“‚ åˆ†ç±»æ•°é‡: {len(stats['category_distribution'])} ä¸ª")
        print(f"   ğŸ“‹ åˆ†ç±»è¯¦æƒ…: {stats['category_distribution']}")
    
    def search_fshd_knowledge(self, question: str, n_results: int = 3, language_filter: str = None):
        """æœç´¢FSHDçŸ¥è¯†åº“"""
        where_filter = None
        if language_filter:
            where_filter = {"language": language_filter}
        
        results = self.collection.query(
            query_texts=[question],
            n_results=n_results,
            where=where_filter
        )
        return results
    
    def search_knowledge(self, question: str, n_results: int = 3, language_filter: str = None):
        """æœç´¢FSHDçŸ¥è¯†åº“ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.search_fshd_knowledge(question, n_results, language_filter)
    
    def get_collection_stats(self):
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        count = self.collection.count()
        all_metadatas = self.collection.get()["metadatas"]
        
        language_dist = {}
        category_dist = {}
        file_type_dist = {}
        
        for meta in all_metadatas:
            lang = meta.get("language", "unknown")
            category = meta.get("category", "unknown")
            file_type = meta.get("file_type", "unknown")
            
            language_dist[lang] = language_dist.get(lang, 0) + 1
            category_dist[category] = category_dist.get(category, 0) + 1
            file_type_dist[file_type] = file_type_dist.get(file_type, 0) + 1
        
        return {
            "total_chunks": count,
            "language_distribution": language_dist,
            "category_distribution": category_dist,
            "file_type_distribution": file_type_dist
        }

def main():
    """ä¸»å‡½æ•° - æ‰¹é‡å¤„ç†æ•´ä¸ªFSHDçŸ¥è¯†åº“"""
    processor = FSHDBatchProcessor()
    
    # çŸ¥è¯†åº“æ ¹è·¯å¾„
    knowledge_base_path = r"C:\yoyo\openrd-master\FSHD_çŸ¥è¯†åº“"
    
    print("ğŸ¯ å¼€å§‹æ‰¹é‡å¤„ç†FSHDçŸ¥è¯†åº“")
    print(f"ğŸ“ çŸ¥è¯†åº“ä½ç½®: {knowledge_base_path}")
    print("â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n")
    
    # æ‰¹é‡å¤„ç†æ•´ä¸ªçŸ¥è¯†åº“
    processor.process_entire_knowledge_base(knowledge_base_path)
    
    # æµ‹è¯•æœç´¢
    print(f"\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
    test_questions = [
        "What is Facioscapulohumeral Muscular Dystrophy?",
        "FSHDçš„è¯Šæ–­æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ",
        "FSHD genetic testing"
    ]
    
    for question in test_questions:
        results = processor.search_fshd_knowledge(question, n_results=2)
        print(f"\nâ“ é—®é¢˜: {question}")
        print(f"ğŸ“‹ æ‰¾åˆ° {len(results['documents'][0])} ä¸ªç›¸å…³ç»“æœ")
        for j, doc in enumerate(results['documents'][0]):
            print(f"   {j+1}. {doc[:150]}...")
            print(f"      åˆ†ç±»: {results['metadatas'][0][j].get('category', 'unknown')}")
            print(f"      è¯­è¨€: {results['metadatas'][0][j].get('language', 'unknown')}")
            print(f"      æ¥æº: {results['metadatas'][0][j].get('source_file', 'unknown')}")

if __name__ == "__main__":
    main()